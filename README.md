# HumanoSync ğŸ¤–

Transform human demonstration videos into robot-compatible data with AI-powered extraction and annotation tools.

## ğŸŒŸ Features

### AI-Powered Extraction
- **MediaPipe Pose Estimation**: Extract 33 body keypoints in 3D
- **YOLOv8 Object Detection**: Identify and track objects in scenes
- **Action Recognition**: Temporal analysis of human movements

### Interactive Annotation Editor
- **Konva.js Canvas**: Drag-and-drop keypoint editing
- **Bounding Box Editor**: Resize and label object detections
- **Action Timeline**: Visual timeline with action segments

### Multi-Format Export
- **JSON**: Structured data for ML pipelines
- **CSV**: Tabular format for analytics
- **YAML**: ROS-compatible format for robotics

## ğŸ› ï¸ Tech Stack

### Backend
- FastAPI (Python 3.8+)
- MediaPipe for pose estimation
- Ultralytics YOLOv8 for object detection
- OpenCV for video processing

### Frontend
- React 18 with React Router
- Konva.js for canvas annotations
- Tailwind CSS for styling
- Axios for API communication

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- Node.js 14+
- npm or yarn

### Backend Setup

1. Navigate to backend directory:
```bash
cd backend
```

2. Create virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Run the server:
```bash
uvicorn main:app --reload --port 8000
```

### Frontend Setup

1. Navigate to frontend directory:
```bash
cd frontend
```

2. Install dependencies:
```bash
npm install
```

3. Start development server:
```bash
npm start
```

The application will be available at:
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Documentation: http://localhost:8000/docs

## ğŸš€ Quick Start

1. **Upload Video**: Navigate to `/upload` and drag-drop your video file
2. **AI Processing**: Wait for automatic extraction (pose, objects, actions)
3. **Edit Annotations**: Use the annotation editor to refine results
4. **Export Data**: Download in JSON, CSV, or YAML format

## ğŸ“ Project Structure

```
humanosync/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ routes/               # API endpoints
â”‚   â”‚   â”œâ”€â”€ upload.py        # Video upload handling
â”‚   â”‚   â”œâ”€â”€ extract.py       # Data extraction endpoints
â”‚   â”‚   â”œâ”€â”€ annotations.py   # Annotation CRUD operations
â”‚   â”‚   â””â”€â”€ export.py        # Export functionality
â”‚   â”œâ”€â”€ services/            # ML model services
â”‚   â”‚   â”œâ”€â”€ pose.py          # MediaPipe pose extraction
â”‚   â”‚   â”œâ”€â”€ objects.py       # YOLOv8 object detection
â”‚   â”‚   â””â”€â”€ actions.py       # Action recognition
â”‚   â”œâ”€â”€ uploads/             # Uploaded videos
â”‚   â””â”€â”€ data/                # Processed annotations
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/           # React pages
â”‚   â”‚   â”‚   â”œâ”€â”€ LandingPage.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadPage.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AnnotatePage.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ExportPage.jsx
â”‚   â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ VideoViewer.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ PoseOverlay.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ BoundingBoxEditor.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ActionTimeline.jsx
â”‚   â”‚   â””â”€â”€ services/        # API service layer
â”‚   â””â”€â”€ public/
â”‚
â””â”€â”€ sample_data/             # Example annotations
```

## ğŸ“Š Data Formats

### Pose Data
```json
{
  "frame_001": {
    "keypoints": {
      "nose": [x, y, z],
      "left_shoulder": [x, y, z],
      ...
    },
    "confidence": 0.94
  }
}
```

### Object Data
```json
{
  "frame_001": [
    {
      "label": "cup",
      "bbox": [x1, y1, x2, y2],
      "confidence": 0.88
    }
  ]
}
```

### Action Data
```json
[
  {
    "label": "pick",
    "start_frame": 32,
    "end_frame": 58,
    "confidence": 0.92
  }
]
```

## ğŸ”Œ API Endpoints

### Upload
- `POST /api/upload` - Upload video file
- `GET /api/video/{video_id}/status` - Check processing status

### Data Extraction
- `GET /api/video/{video_id}/pose` - Get pose data
- `GET /api/video/{video_id}/objects` - Get object detections
- `GET /api/video/{video_id}/actions` - Get action timeline

### Annotations
- `POST /api/video/{video_id}/annotations` - Save all annotations
- `PUT /api/video/{video_id}/pose/{frame_id}` - Update pose frame
- `PUT /api/video/{video_id}/objects/{frame_id}` - Update objects frame

### Export
- `GET /api/video/{video_id}/export?format=json|csv|yaml` - Export data

## âš™ï¸ Configuration

### Environment Variables
Create a `.env` file in the root directory:

```env
REACT_APP_API_URL=http://localhost:8000
BACKEND_PORT=8000
FRONTEND_PORT=3000
```

## ğŸ§ª Testing

### Backend Tests
```bash
cd backend
pytest tests/
```

### Frontend Tests
```bash
cd frontend
npm test
```

## ğŸš¦ Performance Notes

- Video processing time depends on video length and resolution
- YOLOv8 model downloads on first run (~25MB)
- MediaPipe processes at ~30 FPS on modern hardware
- Recommended video resolution: 720p or lower for faster processing

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions are welcome! Please read CONTRIBUTING.md for guidelines.

## ğŸ› Known Issues

- Large videos (>500MB) may timeout during upload
- Some keypoints may be occluded in complex scenes
- Action recognition is rule-based (not using MMAction2 yet)

## ğŸ”® Future Enhancements

- [ ] MMAction2 integration for better action recognition
- [ ] 3D visualization of pose data
- [ ] Multi-video batch processing
- [ ] Cloud storage integration (S3, GCS)
- [ ] Real-time collaboration features
- [ ] Custom robot format templates
- [ ] Fine-tuned models for specific tasks

## ğŸ“§ Support

For issues and questions:
- Create an issue on GitHub
- Email: support@humanosync.ai

## ğŸ™ Acknowledgments

- MediaPipe team for pose estimation
- Ultralytics for YOLOv8
- Konva.js for canvas rendering
- FastAPI for the excellent framework

---

Built with â¤ï¸ for the robotics community